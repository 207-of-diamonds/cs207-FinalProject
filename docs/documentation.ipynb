{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-EW0E8Qe_MB"
   },
   "source": [
    "# **Introduction**\n",
    "\n",
    "In the mid-17th century, Isaac Newton and Gottfried Leibniz independently discovered calculus. These groundbreaking scientific advances unfortunately led to a bitter dispute between the two that spanned the duration of their lives.$^1$ While the origins of calculus may be disputed, its applications are not. Differentiation allows us to identify the maxima, minima and zeros for that function. The ability to do each of these things is crucial in the context of optimization and modern machine learning.\n",
    "\n",
    "The classical way of computing the derivative of a function spans two distinct approaches: approximate, numerical methods and symbolic methods. Each of these methods suffers from its own pitfalls, including numerical instability and long computation time. These pitfalls are magnified as our functions increase in complexity.$^2$ Automatic Differentiation (AD) suffers from neither instability nor long computational time, and it computes derivatives up to machine precision. AD is easily implemented via computer code, and our package will allow the user to implement to forward mode of Automatic Differentiation in python.$^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCGVORb5fWbB"
   },
   "source": [
    "# **Background**\n",
    "\n",
    "\n",
    "### ***What is automatic differentiation (AD)?***\n",
    "\n",
    "\n",
    "Automatic differentiation (AD) is also known as algorithmic differentiation or computational differentiation.$^3$\n",
    "\n",
    "AD is a set of techniques for numerically evaluating derivatives (gradients) by executing a sequence of arithmetic operations and elementary functions. The derivatives can therefore be computed automatically when applying chain rules to such a sequence of operations.$^{4,5}$ There are two major modes of AD: forward and reverse.$^{3,4,5}$ \n",
    "\n",
    "### ***Why is AD important?***\n",
    "\n",
    "AD and symbolic differentiation both result in more accurate computation than numerical difference estimations. However, unlike symbolic approach, AD evaluates expressions numerically at particular numeric values and does not construct symbolic expressions.$^{3}$\n",
    "\n",
    "### ***How AD works?***\n",
    "\n",
    "The core of AD is the chain rule from Calculus. Chain rule computes the derivative for composition of two or more functions, where the derivative of a function measures the change of the output value relative to the change in input.\n",
    "For a composition of f and h, its derivative can be calculated as the following:\n",
    "        $$\\frac{d}{dx}[f(h(x))] = f'(h(x))h'(x)$$\n",
    "\n",
    "With the help of a computational graph and its traces, partial derivatives relative to x and y are combinations of derivatives of elementary functions, which can be calculated analytically.\n",
    "Let’s consider a simple example: $$f = xcos(y)+xy$$\n",
    "In AD, its computational graph and evaluation trace for forward mode looks like the following:\n",
    "\n",
    "### **Computational Graph:**\n",
    "\n",
    "### **Evaluation Trace:**\n",
    "\n",
    "| Trace   | Elementary Function      | Current Value                   | Elementary Function Derivative       | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  |\n",
    "| :---: | :-----------------: | :----------------------: | :----------------------------: | :-----------------:  | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$                  | $x$                     | $\\dot{x}_{1}$                     | $1$ | $0$ |\n",
    "| $x_{2}$ | $x_{2}$                  | $y$                     | $\\dot{x}_{2}$                     | $0$ | $1$ |\n",
    "| $x_{3}$ | $cos(x_{2})$             | $cos(y)$                | $-sin(x_{2})\\dot{x}_{2}$          | $0$ | $-sin(y)$ |\n",
    "| $x_{4}$ | $x_{1}x_{2}$             | $xy$           | $\\dot{x}_{1}x_{2}+x_{1}\\dot{x}_{2}$      | $y$ | $x$ |\n",
    "| $x_{5}$ | $x_{1}cos(x_{2})$        | $xcos(y)$       | $\\dot{x}_{1}(-sin(x_{2}))+x_{1}\\dot{x}_{2}$|$-sin(y)$ | $x$ |\n",
    "| $x_{6}$ | $x_{3}+x_{5}$        | $cos(y)+xcos(y)$       | $\\dot{x}_{3}+\\dot{x}_{5}$    | $-sin(y)$ | $-sin(y)+x$ |\n",
    "\n",
    "\n",
    "We found that $$\\frac{d}{dx}f=-sin(y)$$ and $$\\frac{d}{dy}f=sin(y)+x$$\n",
    "In forward mode, the values and their derivatives are stored along the chain accumulatively.\n",
    "\n",
    "### ***Jacobian and Vectors***\n",
    "\n",
    "The Jacobian is the matrix of partial derivatives.\n",
    "Suppose we have a function $\\mathbf{y}(\\mathbf{x})$ that maps $R_n$ to $R_m$, the Jacobian maxtrix of the function is then the following:\n",
    "$$\n",
    "\\mathbf{J} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\mathbf{f}}{\\partial x_1}  \\dots  \\frac{\\partial \\mathbf{f}}{\\partial x_n}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "\\frac{\\partial {f_1}}{\\partial x_1}  \\dots  \\frac{\\partial {f_1}}{\\partial x_n} \\\\\n",
    "\\vdots  \\ddots  \\vdots\\\\\n",
    "\\frac{\\partial {f_m}}{\\partial x_1}  \\dots  \\frac{\\partial {f_m}}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "In general, for example, we have a function $h(\\mathbf{f}(\\mathbf{x}))$. Suppose a vector $\\mathbf{v}$ happens to be the gradient of h with respect the vector $\\mathbf{f}$ as follows:\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix}\n",
    "\\frac{\\partial h}{\\partial f_1}  \\dots  \\frac{\\partial h}{\\partial f_m}\n",
    "\\end{bmatrix} ^T\n",
    "$$ To get the gradient of h with respect to $\\mathbf{x}$, we multiply Jacobian matrix $\\mathbf{J}$ with vector $\\mathbf{v}$:\n",
    "$$\\mathbf{J} \\cdot \\mathbf{v} = \\begin{bmatrix} \n",
    "\\frac{\\partial {f_1}}{\\partial x_1}  \\dots  \\frac{\\partial {f_m}}{\\partial x_n} \\\\\n",
    "\\vdots  \\ddots  \\vdots\\\\\n",
    "\\frac{\\partial {f_1}}{\\partial x_1}  \\dots  \\frac{\\partial {f_m}}{\\partial x_n} \\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial h}{\\partial y_1} \\\\\n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial h}{\\partial y_m}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial h}{\\partial x_1} \\\\\n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial h}{\\partial x_n}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "### ***Applications of AD:***\n",
    "\n",
    "AD has been used in many applications, including optimization (solving nonlinear equations utilizing gradients/Hessians), inverse problems/data assimilation, neural networks, etc.$^{6}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd2JqNHvf3bI"
   },
   "source": [
    "# How to Use BitterDispute \n",
    "\n",
    "## Installation\n",
    "\n",
    "You can download our package from the PyPI network using pip.\n",
    "\n",
    "    pip install bitterdispute\n",
    "    \n",
    "If you would prefer to copy the repository as is, you can do so here.\n",
    "\n",
    "    git clone https://github.com/207-of-diamonds/cs207-FinalProject.git\n",
    "\n",
    "**Note**: *We recommend performing all tests of BitterDispute in a custom conda environment to ensure required packages remain isolated from your normal space. At the moment, BitterDispute is only available for Python 3. To create a conda environment with the necessary Python version, you can run the below command:*\n",
    "\n",
    "    conda create --name bitterdispute python=3.7\n",
    "    conda activate bitterdispute\n",
    "\n",
    "After cloning the repository, the user should install the minimum requirements using the provided requirements.txt file. To do this, run the below command:\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "## Usage\n",
    "\n",
    "After installing BitterDispute and the necessary dependencies, you can initiate a Python session through the Terminal (Terminal command: *‘python’*) or a Jupyter Notebook session (Terminal command: *‘jupyter notebook’*).\n",
    "\n",
    "The implementation for Forward Mode Automatic Differentiation can be imported using the below command:\n",
    "\n",
    "    from bitterdispute.forward_mode import AD\n",
    "\n",
    "To begin differentiating, simply type **AD()**! You can save this to a variable for future reference and updating of its outputs.\n",
    "\n",
    "    # Initiates forward mode automatic differentiation without saving output values\n",
    "    AD()\n",
    "\n",
    "    # Initiates AD and saves output values to variable x for future reference\n",
    "    x = AD()\n",
    "\n",
    "Our package's user interface will guide you through the process by asking you to enter several values:\n",
    "- The number of variables that are in your formula(s)\n",
    "- Values for each variable\n",
    "- The formula(s) you would like to derive, entered as a copied list or manually typed separated by commas\n",
    "\n",
    "From there, BitterDispute will handle the rest! At the end, the object will print a statement summarizing the formula(s) used and return to output the derived value based on the inputted numbers.\n",
    "\n",
    "There are multiple callable properties for the user to call when they need to retrieve specific information about the input formula and the derived value or formula. The package will return three outputs:\n",
    "- **x.inputs**: The input values for the variables in your formula(s)\n",
    "- **x.outputs**: The ultimate value(s) of the formula(s) based on the values you entered\n",
    "- **x.derivatives**: The final derivative(s) of the formula(s) based on the values you entered\n",
    "- **x.second_derivatives**: The final derivative(s) of the formula(s) based on the values you entered\n",
    "- **x.formulas**: The formula(s) you entered as a string\n",
    "\n",
    "Printing the object 'x' will return a string clarifying which formula was used in this AD() instance.\n",
    "\n",
    "\n",
    "For a printed example of BitterDispute, please see the below example.\n",
    "\n",
    "    >>> from bitterdispute.forward_mode import AD\n",
    "    >>> x = AD()\n",
    "\n",
    "            Welcome to Bitter Dispute! America's favorite automatically\n",
    "            differentiating gameshow. Let's get started.\n",
    "            \n",
    "    How many variables are in your formula? Your limit is 26. >> 2\n",
    "    Great, we'll use 2 variables.\n",
    "    Please enter a value for variable number 1.\n",
    "    y = 5\n",
    "    Please enter a value for variable number 2.\n",
    "    x = 8\n",
    "    Thank you, we recorded these values:\n",
    "    y = 5.0\n",
    "    x = 8.0\n",
    "    Lastly, what formula would you like to derive? Please use the variables just listed.\n",
    "    sin(x)*7+4*tanh(y)\n",
    "    The final derivative of formula sin(x)*7+4*tanh(y) with your chosen values is -1.0177739037365197. Thanks for playing!\n",
    "    >>> x.inputs\n",
    "    ['y = 5.0', 'x = 8.0']\n",
    "    >>> x.val\n",
    "    10.925144543414053\n",
    "    >>> x.der\n",
    "    -1.0177739037365197\n",
    "    >>> x.formula\n",
    "    'sin(x)*7+4*tanh(y)'\n",
    "    >>> print(x)\n",
    "    Formula(s) saved: {self.formulas},\n",
    "        Value(s) used: {self.values},\n",
    "        Derivatives found: {self.derivatives}\n",
    "        \n",
    "        \n",
    "If you'd like to try "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Welcome to Bitter Dispute! America's favorite automatically\n",
      "        differentiating gameshow. Let's get started.\n",
      "        \n",
      "How many variables are in your formula(s)? The limit is 26. >> 2\n",
      "Great, we'll use 2 variables.\n",
      "Please enter a value for variable number 1.\n",
      "y = 1\n",
      "Please enter a value for variable number 2.\n",
      "x = 2\n",
      "Thank you, we recorded these values:\n",
      "y = 1.0\n",
      "x = 2.0\n",
      "Lastly, what formulas would you like to derive? Please use the variables just listed.        You may enter as many formulas as you'd like as a list!\n",
      "x+y\n",
      "The final derivative of formula ['x+y'] with your chosen values is [2]. Thanks for playing!\n",
      "Formula(s) used: ['x+y']\n"
     ]
    }
   ],
   "source": [
    "from bitterdispute.forward_mode import AD\n",
    "x = AD()\n",
    "# Note: You'll be able to enter your own values and formulas once you've installed the package.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formula(s) used: ['x+y']\n",
      "---------------------------\n",
      "['y = 1.0', 'x = 2.0']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AD' object has no attribute 'outputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c3e39ff408d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# View saved information - Detailed Information from each saved parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_derivatives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AD' object has no attribute 'outputs'"
     ]
    }
   ],
   "source": [
    "# View saved information - General Information from print()\n",
    "print(x)\n",
    "print(\"---------------------------\")\n",
    "# View saved information - Detailed Information from each saved parameter\n",
    "print(x.inputs)\n",
    "print(x.outputs)\n",
    "print(x.derivatives)\n",
    "print(x.second_derivatives)\n",
    "print(x.formulas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu_rwCISgFQE"
   },
   "source": [
    "**Software Organization**\n",
    "\n",
    "*What does the directory structure look like?*\n",
    "\n",
    "    LICENSE\n",
    "    README.md\n",
    "    requirements.txt\n",
    "    setup.py\n",
    "    __init__.py\n",
    "    /bitterdispute\n",
    "        forward_mode.py\n",
    "        variable.py\n",
    "        elementary_functions.py\n",
    "        optimization_GD_LS.py\n",
    "    /tests\n",
    "        test_forward_mode.py\n",
    "        test_variable.py\n",
    "        test_elementary_functions.py\n",
    "    /docs\n",
    "        documentation.ipynbd\n",
    "        milestone2.ipynb\n",
    "        milestone1.ipynb\n",
    "        BitterDispute_ADGraph.png\n",
    "    /dist\n",
    "        bitterdispute-0.1.tar.gz\n",
    "        bitterdispute-0.1-py3-none-any.whl\n",
    "    /build\n",
    "        bdist.macosx-10.7-x86_64\n",
    "    bitterdispute.egg-info\n",
    " \n",
    "*What modules are included? What is their basic functionality?*\n",
    "\n",
    "Within our package, */bitterdispute* contains the forward mode user-facing implementation, our variable class, mathematical elementary functions for our variable class and functions defining various optimization methods. Our forward mode module imports all necessary classes and functions to execute forward mode automatic differentiation.\n",
    "\n",
    "*Where does your test suite live?*\n",
    "\n",
    "Our tests live in a dedicated module to assist with robust test creation. In */tests*, we maintain around 50todo testing functions for the various .py files that comprise our BitterDispute package. We use both Travis-CI and CodeCov to monitor our commits and to ensure that keeping our tests in a separate module doesn’t prevent us from maintaining sufficient code coverage.\n",
    "\n",
    "To run tests manually, from the repository main folder, simply run one or both of the below commands:\n",
    "\n",
    "    pytest                        # Gathers all tests in the repository and runs simultaneously\n",
    "    pytest --cov=bitterdispute    # Gathers all tests and calculates repository code coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbWP9gQ9K5JS"
   },
   "source": [
    "**Implementation**\n",
    ">*AD Class*\n",
    "\n",
    "We have implemented the AD class, which provides the user-facing instructions for forward mode automatic differentiation. It seeks input from the user's screen and saves that in Variable class objects.\n",
    "\n",
    "To initialize the Automatic Differentation class, the user will call AD() by itself or saved to a variable and follow the on-screen instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbWP9gQ9K5JS"
   },
   "source": [
    ">*Variable Class*\n",
    "\n",
    "We have implemented the Variable class, which will take in scalar variables. Each Variable is meant to represent one scalar variable ('x') at a time, but dynamically tracks derivatives and second derivatives with respect to other Variables it interacts with in any formula.\n",
    "\n",
    "To initialize the Variable class, the user will pass a name as stirng and value as int or float for the variable. The .name, .val, .der and .der2 attributes of the Variable class object can be called to return the name of the variable, its current value, its derivatives and second derivatives with respect to any variables it encounters after instantiation within a formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "10oIXKboLFK9",
    "outputId": "7eabcf1e-6a6c-48f6-9a7f-dba72f69e37a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bitterdispute.variable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-681ef8ac9a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbitterdispute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbitterdispute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_mode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitterdispute.variable'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from bitterdispute.variable import Variable\n",
    "from bitterdispute.forward_mode import AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1it4ShLH3y"
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "a = 2.0 \n",
    "x = Variable('x', a)\n",
    "alpha = 2.0\n",
    "beta = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "c6x2-9yPLLOP",
    "outputId": "4a77abf5-6823-4e07-822f-e6a21c2c38ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha + x:  4.0 ; Dervative:  1\n",
      "Value of x + alpha:  4.0 ; Dervative:  1\n"
     ]
    }
   ],
   "source": [
    "#Addition\n",
    "f = alpha + x\n",
    "print(\"Value of alpha + x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x + alpha\n",
    "print(\"Value of x + alpha: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "lJsjDw66LOOF",
    "outputId": "6fb5abf0-fc36-4e01-f2b2-c5e4f701b915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta - x:  1.0 ; Dervative:  -1\n",
      "Value x - beta:  -1.0 ; Dervative:  1\n"
     ]
    }
   ],
   "source": [
    "#Subtraction\n",
    "f = beta - x\n",
    "print(\"Value of beta - x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x - beta\n",
    "print(\"Value x - beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nRncuGCoLRPs",
    "outputId": "14bc1dde-a4f1-4693-9d36-9ce5d656b0ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha*x:  4.0 ; Dervative:  2.0\n",
      "Value x*beta:  6.0 ; Dervative:  3.0\n"
     ]
    }
   ],
   "source": [
    "#Multiplication\n",
    "f = alpha*x\n",
    "print(\"Value of alpha*x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x*beta\n",
    "print(\"Value x*beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "8jqUoFH3LWZk",
    "outputId": "be65f77e-ad1a-422b-e4b2-a18dc6d62045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha/x:  1.0 ; Dervative:  -0.5\n",
      "Value x/beta:  0.6666666666666666 ; Dervative:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Division\n",
    "f = alpha/x\n",
    "print(\"Value of alpha/x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x/beta\n",
    "print(\"Value x/beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "4lBpxD8DLZwt",
    "outputId": "4b729919-1850-4fe2-e210-b4bdd48861e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha**x:  4.0 ; Dervative:  2.772588722239781\n",
      "Value x**beta:  8.0 ; Dervative:  12.0\n"
     ]
    }
   ],
   "source": [
    "#Power\n",
    "f = alpha**x\n",
    "print(\"Value of alpha**x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x**beta\n",
    "print(\"Value x**beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3bfX8VuqLu8"
   },
   "source": [
    ">*Elementary Functions:*\n",
    "\n",
    "We've defined a series of elementary operations on Variable objects or constant values, including trig, exponential, and log functions.\n",
    "\n",
    "Our module relies on the numpy package to evaluate the above elementary functions. The methods in the func module define how to perform elementary operations on scalars as well as variable objects.\n",
    "\n",
    "For example, the sin function uses numpy.sin and numpy.cos to calculate value and derivative respectively. When a scalar input is detected, it is given a null derivative (i.e. treated as a constant). Sin returns a variable object with the updated value and derivative (perhaps zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnbWyNfdqSsW"
   },
   "outputs": [],
   "source": [
    "#import our modules\n",
    "import bitterdispute.elementary_functions as elem\n",
    "import bitterdispute.variable as var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UykuhCaKr2sH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of sin(x): 0.8414709848078965\n",
      "Derivative of sin(x): 0.5403023058681398\n"
     ]
    }
   ],
   "source": [
    "#Example:\n",
    "val=1\n",
    "#instantiate a Variable object\n",
    "x=Variable('x', val)\n",
    "#test_sin\n",
    "obj = elem.sin(x)\n",
    "print(\"Value of sin(x):\", obj.val)\n",
    "print(\"Derivative of sin(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_qfDc16sbW1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of cos(x): 0.5403023058681398\n",
      "Derivative of cos(x): -0.8414709848078965\n"
     ]
    }
   ],
   "source": [
    "#test_cos\n",
    "obj=elem.cos(x)\n",
    "print(\"Value of cos(x):\", obj.val)\n",
    "print(\"Derivative of cos(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afaWj2Jmshnn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of tan(x): 1.557407724654902\n",
      "Derivative of tan(x): 3.425518820814759\n"
     ]
    }
   ],
   "source": [
    "#test_tan\n",
    "obj=elem.tan(x)\n",
    "print(\"Value of tan(x):\", obj.val)\n",
    "print(\"Derivative of tan(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cm1ihGVTsn5q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of exp(x): 2.718281828459045\n",
      "Derivative of exp(x): 2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "#test_exponential\n",
    "obj=elem.exp(x)\n",
    "print(\"Value of exp(x):\", obj.val)\n",
    "print(\"Derivative of exp(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jSgEOvcs8kl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of ln(x): 0.0\n",
      "Derivative of ln(x): 1.0\n"
     ]
    }
   ],
   "source": [
    "#test_log\n",
    "obj=elem.log(x) # this directly computes the ln(); elem.log(x,base) should allow any base to work.\n",
    "print(\"Value of ln(x):\", obj.val)\n",
    "print(\"Derivative of ln(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRIZ9ZF89NNV"
   },
   "source": [
    "\n",
    "What method and name attributes will your classes have?\n",
    "\n",
    "* Methods for Variable Class\n",
    "\n",
    "```\n",
    "__add__\n",
    "__radd__\n",
    "__sub__\n",
    "__rsub__\n",
    "__mul__\n",
    "__rmul__\n",
    "__truediv__\n",
    "__rtruediv__\n",
    "__pow__\n",
    "__rpow__\n",
    "__neg__\n",
    "\n",
    "``` \n",
    "What external dependencies will you rely on?\n",
    "\n",
    "\n",
    "```\n",
    "Numpy\n",
    "\n",
    "Pytest\n",
    "\n",
    "Pytest-mock\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdBwsz52IeAV"
   },
   "source": [
    "How will you deal with elementary functions like ***sin, sqrt, log, and exp*** (and all the others)?\n",
    "\n",
    "> Elementary functions: \n",
    "\n",
    "*   sin, cos, tan\n",
    "*   log (logarithm of any chosen base)\n",
    "*   exp\n",
    "*   arcsin, arccos, arctan \n",
    "*   sinh, cosh, tanh\n",
    "*   sqrt\n",
    "\n",
    "Our module relies on the NumPy package to evaluate the above elementary functions. The methods in the func module define how to perform elementary operations on scalars as well as variable objects.\n",
    "\n",
    "For example, the sin function uses numpy.sin and numpy.cos to calculate value and derivative respectively. When a scalar input is detected, it is given a null derivative (i.e. treated as a constant). Sin returns a variable object with the updated value and derivative (perhaps zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32vwQM5TO8-s"
   },
   "source": [
    "# Optimization Methods\n",
    "\n",
    "To motivate the importance of automatic differentiation, BitterDispute comes with several optimization functions included. A user can find the optimum point using everyone's favorite optimizations such as Newton's Method, Gradient Descent, and Backtracking Line Search.\n",
    "\n",
    "**Goal**: Find the minimizer $x^*$ that minimizes the objective function $f(x)$, such that $\\nabla f(x^*)=0$\n",
    "$$x^* = argmin\\ f(x)$$ \n",
    "\n",
    "To find a valid solution on optimization problems, we want to introduce a second-derivative method to our software package: **Newton's method** \n",
    "\n",
    "Recall that Taylor expansion is: \n",
    "$$f(x+h) \\approx f(x) + f'(x)h$$\n",
    "\n",
    "Here we have a nonlinear function that can be estimated using linear approximation close to x\n",
    "\n",
    "$f'(x+h) = 0$ , that is  $f'(x) + f''(x)h = 0$, that is $h = -\\frac{f'(x)}{f''(x)}$\n",
    "\n",
    "**Algorithm in 1-Dimension:**\n",
    "\n",
    "$$x_0 = initial\\ guess$$\n",
    "$$h = -\\frac{f'(x)}{f''(x)}$$\n",
    "\n",
    "$$x_{k+1}=x_{k} + h = x_{k}-\\frac{f'(x)}{f''(x)}$$\n",
    "\n",
    "**Algorithm in *N*-Dimension:**\n",
    "$$x_0 = initial\\ guess$$\n",
    "Solve $$J_k s_k = -f(x_k)$$ where $J_k$ is the Jacobian matrix, and $(J)_{ij} = \\frac{\\partial f_i}{\\partial x_j}$\n",
    "\n",
    "Update $$x_{k+1}=x_{k} + s_k$$\n",
    "\n",
    "The module of Newton's method can be used as shown below:\n",
    "```python\n",
    "def funct(values):\n",
    "        var = Variable(name='x', value=values)\n",
    "        f = (var-5)**2\n",
    "        return f\n",
    "a=Newton(funct, x0, iter_max=100, error_max=1e-5)\n",
    "\n",
    "```\n",
    "Similarily, the Quasi Newton's method is based on Newton's method to find the stationary point of a function, where the gradient is 0, and can be used as the following:\n",
    "```python\n",
    "def funct(values):\n",
    "        var = Variable(name='x', value=values)\n",
    "        f = (var-5)**2\n",
    "        return f\n",
    "a=Quasi_Newton(funct, x0, iter_max=100, error_max=1e-5)\n",
    "```\n",
    "**Convergence**\n",
    "- Quadratic convergence\n",
    "- May fail and reach a local convergence\n",
    "\n",
    "**Gradient Descent:** \n",
    "\n",
    "For gradient descent, in each iteration the step is defined by $\\Delta x_k = - stepsize * \\nabla f(x_k)$. \n",
    "The user can change the initial guesses, step size, max number of iterations, and tolerance.\n",
    "\n",
    "\n",
    "The module can be used as shown below:\n",
    "```python\n",
    ">>> def f(val):\n",
    "...     x1 = Variable('x',val)\n",
    "...     f = (x1-5) ** 2\n",
    "...    return f\n",
    ">>> a = gradientDescent(f,init_val=[3],lr=0.01, max_iters=10000, tol=1e-4)\n",
    "\n",
    "\" Total iterations, 298, and the local minimum occurs at 4.99514263709298.\"\n",
    "```\n",
    "\n",
    "\n",
    "**Backtracking Line Search:** \n",
    "\n",
    "It is one of the line search methods used to determine the maximum amount to move along a given search direction.\n",
    "\n",
    "\n",
    "Starting with a maximum candidate step size value $\\alpha$>0, using search control parameters $\\tau$ and $c$, the backtracking line search algorithm reduce $\\alpha$ by a factor of $\\tau$, in each iteration until the Armijo–Goldstein condition:  $f(x)- f(x + \\alpha_i *p) \\geq \\alpha_i *t$, where $t=-cm$, is fulfilled\n",
    "\n",
    " \n",
    "The module can be used as shown below:\n",
    "```python\n",
    ">>> def f(val):\n",
    "...    x1 = Variable('x',val[0])\n",
    "...    f = (x1-5) ** 2\n",
    "...    return f\n",
    ">>> a = backtrackingLineSearch(f, init_val=[7])\n",
    ">>> b = backtrackingLineSearch(f, init_val=[5])\n",
    "\"Total iterations 1, the alpha, step size, is 0.1\"\n",
    "\"Total iterations 9, the alpha, step size, is 1.0000000000000002e-17\"\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNOX_heyvAgK"
   },
   "source": [
    "**References**\n",
    "\n",
    "[1] Derivative. Wikipedia. Available at: https://en.wikipedia.org/wiki/Derivative#History\n",
    "\n",
    "[2] Hoffman, Philipp H.W. “A Hitchhiker’s Guide to Automatic Differentiation.” Numerical Algorithms, 72, 24 October 2015, 775-811, Springer Link, DOI 10.1007/s11075-015-0067-6. \n",
    "\n",
    "[3] Automatic differentiation. Wikipedia. Available at https://en.wikipedia.org/wiki/Automatic_differentiation.\n",
    "\n",
    "[4] Griewank, A., in Complexity in Nonlinear Optimization (ed. Pardalos, P.), World Scientific, Singapore, 1993, pp. 128–161.\n",
    "\n",
    "[5] Coleman, T. F. and Verma, A., in Computational Differentiation: Techniques, Applications and Tools (eds Berz, M., Bischof, C., Corliss, G. and Griewank, A.), SIAM, Philadelphia, 1966, pp. 149–159.\n",
    "\n",
    "[6] Andreas Griewank: Evaluating Derivatives. SIAM 2000.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " 207 of Diamonds - Milestone 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
