{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-EW0E8Qe_MB"
   },
   "source": [
    "**Introduction**\n",
    "\n",
    "In the mid-17th century, Isaac Newton and Gottfried Leibniz independently discovered calculus. These groundbreaking scientific advances unfortunately led to a bitter dispute between the two that spanned the duration of their lives. [1] While the origins of calculus may be disputed, its applications are not. Differentiation allows us to identify the maxima, minima and zeros for that function. The ability to do each of these things is crucial in the context of optimization and modern machine learning.\n",
    "\n",
    "The classical way of computing the derivative of a function spans two distinct approaches: approximate, numerical methods and symbolic methods. Each of these methods suffers from its own pitfalls, including numerical instability and long computation time. These pitfalls are magnified as our functions increase in complexity. [2] Automatic Differentiation (AD) suffers from neither instability nor long computational time, and it computes derivatives up to machine precision. AD is easily implemented via computer code, and our package will allow the user to implement to forward mode of Automatic Differentiation in python. [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCGVORb5fWbB"
   },
   "source": [
    "**Background**\n",
    "\n",
    "\n",
    "***What is automatic differentiation (AD)?***\n",
    "\n",
    "\n",
    "Automatic differentiation (AD) is also known as algorithmic differentiation or computational differentiation [3].\n",
    "\n",
    "AD is a set of techniques for numerically evaluating derivatives (gradients) by executing a sequence of arithmetic operations and elementary functions. The derivatives can therefore be computed automatically when applying chain rules to such a sequence of operations [4-5]. There are two major modes of AD: forward and reverse [3-5]. \n",
    "\n",
    "***Why is AD important?***\n",
    "\n",
    "AD and symbolic differentiation both result in more accurate computation than numerical difference estimations. However, unlike symbolic approach, AD evaluates expressions numerically at particular numeric values and does not construct symbolic expressions [3].\n",
    "\n",
    "***How AD works?***\n",
    "\n",
    "The core of AD is the chain rule from Calculus. Chain rule computes the derivative for composition of two or more functions, where the derivative of a function measures the change of the output value relative to the change in input.\n",
    "For a composition of f and h, its derivative can be calculated as the following:\n",
    "        $$\\frac{d}{dx}[f(h(x))] = f'(h(x))h'(x)$$\n",
    "\n",
    "With the help of a computational graph and its traces, partial derivatives relative to x and y are combinations of derivatives of elementary functions, which can be calculated analytically.\n",
    "Let’s consider a simple example: $$f = xcos(y)+xy$$\n",
    "In AD, its computational graph and evaluation trace for forward mode looks like the following:\n",
    "\n",
    "**Computational Graph:**\n",
    "\n",
    "**Evaluation Trace:**\n",
    "\n",
    "| Trace   | Elementary Function      | Current Value                   | Elementary Function Derivative       | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  |\n",
    "| :---: | :-----------------: | :----------------------: | :----------------------------: | :-----------------:  | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$                  | $x$                     | $\\dot{x}_{1}$                     | $1$ | $0$ |\n",
    "| $x_{2}$ | $x_{2}$                  | $y$                     | $\\dot{x}_{2}$                     | $0$ | $1$ |\n",
    "| $x_{3}$ | $cos(x_{2})$             | $cos(y)$                | $-sin(x_{2})\\dot{x}_{2}$          | $0$ | $-sin(y)$ |\n",
    "| $x_{4}$ | $x_{1}x_{2}$             | $xy$           | $\\dot{x}_{1}x_{2}+x_{1}\\dot{x}_{2}$      | $y$ | $x$ |\n",
    "| $x_{5}$ | $x_{1}cos(x_{2})$        | $xcos(y)$       | $\\dot{x}_{1}(-sin(x_{2}))+x_{1}\\dot{x}_{2}$|$-sin(y)$ | $x$ |\n",
    "| $x_{6}$ | $x_{3}+x_{5}$        | $cos(y)+xcos(y)$       | $\\dot{x}_{3}+\\dot{x}_{5}$    | $-sin(y)$ | $-sin(y)+x$ |\n",
    "\n",
    "\n",
    "We found that $$\\frac{d}{dx}f=-sin(y)$$ and $$\\frac{d}{dy}f=sin(y)+x$$\n",
    "In forward mode, the values and their derivatives are stored along the chain accumulatively.\n",
    "\n",
    "***Jacobian and Vectors***\n",
    "\n",
    "The Jacobian is the matrix of partial derivatives.\n",
    "Suppose we have a function $\\mathbf{y}(\\mathbf{x})$ that maps $R_n$ to $R_m$, the Jacobian maxtrix of the function is then the following:\n",
    "$$\n",
    "\\mathbf{J} = \\begin{bmatrix}\n",
    "\\frac{\\partial \\mathbf{f}}{\\partial x_1}  \\dots  \\frac{\\partial \\mathbf{f}}{\\partial x_n}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "\\frac{\\partial {f_1}}{\\partial x_1}  \\dots  \\frac{\\partial {f_1}}{\\partial x_n} \\\\\n",
    "\\vdots  \\ddots  \\vdots\\\\\n",
    "\\frac{\\partial {f_m}}{\\partial x_1}  \\dots  \\frac{\\partial {f_m}}{\\partial x_n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "In general, for example, we have a function $h(\\mathbf{f}(\\mathbf{x}))$. Suppose a vector $\\mathbf{v}$ happens to be the gradient of h with respect the vector $\\mathbf{f}$ as follows:\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix}\n",
    "\\frac{\\partial h}{\\partial f_1}  \\dots  \\frac{\\partial h}{\\partial f_m}\n",
    "\\end{bmatrix} ^T\n",
    "$$ To get the gradient of h with respect to $\\mathbf{x}$, we multiply Jacobian matrix $\\mathbf{J}$ with vector $\\mathbf{v}$:\n",
    "$$\\mathbf{J} \\cdot \\mathbf{v} = \\begin{bmatrix} \n",
    "\\frac{\\partial {f_1}}{\\partial x_1}  \\dots  \\frac{\\partial {f_m}}{\\partial x_n} \\\\\n",
    "\\vdots  \\ddots  \\vdots\\\\\n",
    "\\frac{\\partial {f_1}}{\\partial x_1}  \\dots  \\frac{\\partial {f_m}}{\\partial x_n} \\\\\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial h}{\\partial y_1} \\\\\n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial h}{\\partial y_m}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\frac{\\partial h}{\\partial x_1} \\\\\n",
    "\\vdots \\\\ \n",
    "\\frac{\\partial h}{\\partial x_n}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "***Applications of AD:***\n",
    "\n",
    "AD has been used in many applications, including optimization (solving nonlinear equations utilizing gradients/Hessians), inverse problems/data assimilation, neural networks, etc. [6]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd2JqNHvf3bI"
   },
   "source": [
    "# How to Use BitterDispute \n",
    "\n",
    "## Installation\n",
    "\n",
    "In future releases, a user will be able to download our package using pip. For now, a user can clone our code repository by running the below command in their Terminal:\n",
    "\n",
    "    git clone https://github.com/207-of-diamonds/cs207-FinalProject.git\n",
    "\n",
    "**Note**: *We recommend performing all tests of BitterDispute in a custom conda environment to ensure required packages remain isolated from your normal space. At the moment, BitterDispute is only available for Python 3. To create a conda environment with the necessary Python version, you can run the below command:*\n",
    "\n",
    "    conda create --name bitterdispute python=3.7\n",
    "\n",
    "After cloning the repository, the user should install the minimum requirements using the provided requirements.txt file. To do this, run the below command:\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "## Usage\n",
    "\n",
    "After cloning BitterDispute and installing the necessary dependencies, you should navigate into the repository's main folder and initiate a Python session through the Terminal (Terminal command: *‘python’*) or a Jupyter Notebook session (Terminal command: *‘jupyter notebook’*).\n",
    "\n",
    "The implementation for Forward Mode Automatic Differentiation can be imported using the below command from the repository main folder level:\n",
    "\n",
    "    from bitterdispute.forward_mode import AD\n",
    "\n",
    "To begin differentiating, simply type AD()! You can save this to a variable for future reference of its outputs.\n",
    "\n",
    "    # Initiates forward mode automatic differentiation without saving output values\n",
    "    AD()\n",
    "\n",
    "    # Initiates AD and saves output values to variable x for future reference\n",
    "    x = AD()\n",
    "\n",
    "Our package will guide you through the process by asking you to enter several values:\n",
    "- The number of variables that are in your formula\n",
    "- Values for each variable\n",
    "- The formula you would like to derive\n",
    "\n",
    "From there, BitterDispute will handle the rest! At the end, the object will print a statement summarizing the formula used and return to output the derived value based on the inputted numbers.\n",
    "\n",
    "There are multiple callable properties for the user to call when they need to retrieve specific information about the input formula and the derived value or formula. The package will return three outputs:\n",
    "- **x.inputs**: The input values for the variables in your formula\n",
    "- **x.val**: The ultimate value of the formula based on the values you entered\n",
    "- **x.der**: The final derivative of the formula based on the values you entered\n",
    "- **x.formula**: The formula you entered as a string\n",
    "\n",
    "Printing the object 'x' will return a string clarifying which formula was used in this AD() instance. For a demonstration of BitterDispute, please see the below example.\n",
    "\n",
    "    >>> from bitterdispute.forward_mode import AD\n",
    "    >>> x = AD()\n",
    "\n",
    "            Welcome to Bitter Dispute! America's favorite automatically\n",
    "            differentiating gameshow. Let's get started.\n",
    "            \n",
    "    How many variables are in your formula? Your limit is 26. >> 2\n",
    "    Great, we'll use 2 variables.\n",
    "    Please enter a value for variable number 1.\n",
    "    y = 5\n",
    "    Please enter a value for variable number 2.\n",
    "    x = 8\n",
    "    Thank you, we recorded these values:\n",
    "    y = 5.0\n",
    "    x = 8.0\n",
    "    Lastly, what formula would you like to derive? Please use the variables just listed.\n",
    "    sin(x)*7+4*tanh(y)\n",
    "    The final derivative of formula sin(x)*7+4*tanh(y) with your chosen values is -1.0177739037365197. Thanks for playing!\n",
    "    >>> x.inputs\n",
    "    ['y = 5.0', 'x = 8.0']\n",
    "    >>> x.val\n",
    "    10.925144543414053\n",
    "    >>> x.der\n",
    "    -1.0177739037365197\n",
    "    >>> x.formula\n",
    "    'sin(x)*7+4*tanh(y)'\n",
    "    >>> print(x)\n",
    "    Formula used is sin(x)*7+4*tanh(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu_rwCISgFQE"
   },
   "source": [
    "**Software Organization**\n",
    "\n",
    "*What does the directory structure look like?*\n",
    "\n",
    "    README.md\n",
    "    requirements.txt\n",
    "    /bitterdispute\n",
    "        forward_mode.py\n",
    "        scalar.py\n",
    "        elementary_functions.py\n",
    "    /tests\n",
    "        test_forward_mode.py\n",
    "        test_scalar.py\n",
    "        test_elementary_functions.py\n",
    "    /docs\n",
    "        milestone1.ipynb\n",
    "        milestone2.ipynb\n",
    " \n",
    "*What modules are included? What is their basic functionality?*\n",
    "\n",
    "Within our package, */bitterdispute* contains the forward mode user-facing implementation, our scalar class and mathematical elementary functions for our scalar class. Our forward mode module imports all necessary classes and functions to execute forward mode automatic differentiation. \n",
    "\n",
    "*Where does your test suite live?*\n",
    "\n",
    "Our tests live in a dedicated module to assist with robust test creation. In */tests*, we maintain around 50 testing functions for the various .py files that comprise our BitterDispute package. We will be using both Travis-CI and CodeCov to monitor our commits and to ensure that keeping our tests in a separate module doesn’t prevent us from maintaining sufficient code coverage.\n",
    "\n",
    "To run tests manually, from the repository main folder, simply run one or both of the below commands:\n",
    "\n",
    "    pytest          # Gathers all tests in the repository and runs simultaneously\n",
    "    pytest --cov    # Gathers all tests and calculates repository code coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbWP9gQ9K5JS"
   },
   "source": [
    "**Implementation**\n",
    ">*AD Class*\n",
    "\n",
    "We have implemented the AD class, which provides the user-facing instructions for forward mode automatic differentiation. It seeks input from the user's screen and saves that in Scalar class objects.\n",
    "\n",
    "To initialize the Scalar class, the user will call AD() by itself or saved to a variable and follow the on-screen instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbWP9gQ9K5JS"
   },
   "source": [
    ">*Scalar Class*\n",
    "\n",
    "We have implemented the Scalar class, which will take in scalar variables. Our implementation currently handles the input of one scalar variable ('x') at a time.\n",
    "\n",
    "To initialize the Scalar class, the user will pass a value for the variable. The .val and .der attributes of the Scalar class object can be called to return the value of the variable and its derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "10oIXKboLFK9",
    "outputId": "7eabcf1e-6a6c-48f6-9a7f-dba72f69e37a"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from bitterdispute.scalar import Scalar\n",
    "from bitterdispute.forward_mode import AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1it4ShLH3y"
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "a = 2.0 \n",
    "x = Scalar(a)\n",
    "alpha = 2.0\n",
    "beta = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "c6x2-9yPLLOP",
    "outputId": "4a77abf5-6823-4e07-822f-e6a21c2c38ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha + x:  4.0 ; Dervative:  1\n",
      "Value of x + alpha:  4.0 ; Dervative:  1\n"
     ]
    }
   ],
   "source": [
    "#Addition\n",
    "f = alpha + x\n",
    "print(\"Value of alpha + x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x + alpha\n",
    "print(\"Value of x + alpha: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "lJsjDw66LOOF",
    "outputId": "6fb5abf0-fc36-4e01-f2b2-c5e4f701b915"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta - x:  1.0 ; Dervative:  -1\n",
      "Value x - beta:  -1.0 ; Dervative:  1\n"
     ]
    }
   ],
   "source": [
    "#Subtraction\n",
    "f = beta - x\n",
    "print(\"Value of beta - x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x - beta\n",
    "print(\"Value x - beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nRncuGCoLRPs",
    "outputId": "14bc1dde-a4f1-4693-9d36-9ce5d656b0ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha*x:  4.0 ; Dervative:  2.0\n",
      "Value x*beta:  6.0 ; Dervative:  3.0\n"
     ]
    }
   ],
   "source": [
    "#Multiplication\n",
    "f = alpha*x\n",
    "print(\"Value of alpha*x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x*beta\n",
    "print(\"Value x*beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "8jqUoFH3LWZk",
    "outputId": "be65f77e-ad1a-422b-e4b2-a18dc6d62045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha/x:  1.0 ; Dervative:  -0.5\n",
      "Value x/beta:  0.6666666666666666 ; Dervative:  0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Division\n",
    "f = alpha/x\n",
    "print(\"Value of alpha/x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x/beta\n",
    "print(\"Value x/beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "4lBpxD8DLZwt",
    "outputId": "4b729919-1850-4fe2-e210-b4bdd48861e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of alpha**x:  4.0 ; Dervative:  2.772588722239781\n",
      "Value x**beta:  8.0 ; Dervative:  12.0\n"
     ]
    }
   ],
   "source": [
    "#Power\n",
    "f = alpha**x\n",
    "print(\"Value of alpha**x: \", f.val,\"; Dervative: \", f.der)\n",
    "f = x**beta\n",
    "print(\"Value x**beta: \", f.val,\"; Dervative: \", f.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3bfX8VuqLu8"
   },
   "source": [
    ">*Elementary Functions:*\n",
    "\n",
    "We've defined a series of elementary operations on Scalar object or constant values, including trig, exponential, and log functions.\n",
    "\n",
    "Our module relies on the numpy package to evaluate the above elementary functions. The methods in the func module define how to perform elementary operations on scalars as well as variable objects.\n",
    "\n",
    "For example, the sin function uses numpy.sin and numpy.cos to calculate value and derivative respectively. When a scalar input is detected, it is given a null derivative (i.e. treated as a constant). Sin returns a variable object with the updated value and derivative (perhaps zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LnbWyNfdqSsW"
   },
   "outputs": [],
   "source": [
    "#import our modules\n",
    "import bitterdispute.elementary_functions as elem\n",
    "import bitterdispute.scalar as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UykuhCaKr2sH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of sin(x): 0.8414709848078965\n",
      "Derivative of sin(x): 0.5403023058681398\n"
     ]
    }
   ],
   "source": [
    "#Example:\n",
    "val=1\n",
    "#instantiate a Scalar object\n",
    "x=Scalar(val)\n",
    "#test_sin\n",
    "obj = elem.sin(x)\n",
    "print(\"Value of sin(x):\", obj.val)\n",
    "print(\"Derivative of sin(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_qfDc16sbW1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of sin(x): 0.5403023058681398\n",
      "Derivative of sin(x): -0.8414709848078965\n"
     ]
    }
   ],
   "source": [
    "#test_cos\n",
    "obj=elem.cos(x)\n",
    "print(\"Value of cos(x):\", obj.val)\n",
    "print(\"Derivative of cos(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afaWj2Jmshnn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of sin(x): 1.557407724654902\n",
      "Derivative of sin(x): 3.425518820814759\n"
     ]
    }
   ],
   "source": [
    "#test_tan\n",
    "obj=elem.tan(x)\n",
    "print(\"Value of tan(x):\", obj.val)\n",
    "print(\"Derivative of tan(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cm1ihGVTsn5q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of sin(x): 2.718281828459045\n",
      "Derivative of sin(x): 2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "#test_exponential\n",
    "obj=elem.exp(x)\n",
    "print(\"Value of exp(x):\", obj.val)\n",
    "print(\"Derivative of exp(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jSgEOvcs8kl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of sin(x): 0.0\n",
      "Derivative of sin(x): 1.0\n"
     ]
    }
   ],
   "source": [
    "#test_log\n",
    "obj=elem.log(x) # this directly computes the ln(); elem.log(x,base) should allow any base to work.\n",
    "print(\"Value of ln(x):\", obj.val)\n",
    "print(\"Derivative of ln(x):\", obj.der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRIZ9ZF89NNV"
   },
   "source": [
    "\n",
    "What method and name attributes will your classes have?\n",
    "\n",
    "* Methods for Scalar Class\n",
    "\n",
    "```\n",
    "__add__\n",
    "__radd__\n",
    "__sub__\n",
    "__rsub__\n",
    "__mul__\n",
    "__rmul__\n",
    "__truediv__\n",
    "__rtruediv__\n",
    "__pow__\n",
    "__rpow__\n",
    "__neg__\n",
    "\n",
    "``` \n",
    "What external dependencies will you rely on?\n",
    "\n",
    "\n",
    "```\n",
    "Numpy\n",
    "\n",
    "Math\n",
    "\n",
    "scipy* # For future consideration\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdBwsz52IeAV"
   },
   "source": [
    "How will you deal with elementary functions like ***sin, sqrt, log, and exp*** (and all the others)?\n",
    "\n",
    "> Elementary functions: \n",
    "\n",
    "*   sin, cos, tan\n",
    "*   log (logarithm of any chosen base)\n",
    "*   exp\n",
    "*   arcsin, arccos, arctan \n",
    "*   sinh, cosh, tanh\n",
    "*   sqrt (to be implemented)\n",
    "\n",
    "Our module relies on the numpy package to evaluate the above elementary functions. The methods in the func module define how to perform elementary operations on scalars as well as variable objects.\n",
    "\n",
    "For example, the sin function uses numpy.sin and numpy.cos to calculate value and derivative respectively. When a scalar input is detected, it is given a null derivative (i.e. treated as a constant). Sin returns a variable object with the updated value and derivative (perhaps zero).\n",
    "\n",
    "See above section for demo of our implementations for sin, cos, tan, exp and log. We have several others already implemented as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wks-s3J8auhv"
   },
   "source": [
    "# Future Features\n",
    "\n",
    "### Improvements\n",
    "There are several ways that we want to improve the existing forward mode implementation to make it a better user experience. First, we want to improve distribution of the package to make it downloadable via pip. Second, we want to provide additional information on the number of trace steps that it took for the formula to calculate the derivative value given the user's inputs. This will help the user's understand one measure of complexity of their input formula.\n",
    "\n",
    "Most significantly, we want to provide the user with the ability to re-use an instance of our package with a saved formula. If a user has saved an instance to a local variable (like X from earlier), the user will be able to call X.derive(x, y) where x and y are two values of type int or float and X is an instantiation of AD with a saved formula of just two variables. Calling an existing instance will update the X.derivative value with the new derived value, keep the saved formula and update the input values saved in X.values. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32vwQM5TO8-s"
   },
   "source": [
    "### **Optimization**\n",
    "\n",
    "We intend to implement an enhancement of our automatic differentiator to better deal with optimization use cases.\n",
    "\n",
    "**Goal**: Find the minimizer $x^*$ that minimizes the objective function $f(x)$, such that $∇f(x^*)=0$\n",
    "$$x^* = argmin\\ f(x)$$ \n",
    "\n",
    "To find a valid solution on optimization problems, we want to introduce a second-derivative method to our software package: **Newton's method** \n",
    "\n",
    "Recall that Taylor expansion is: \n",
    "$$f(x+h) ≈ f(x) + f'(x)h$$\n",
    "\n",
    "Here we have a nonlinear function that can be estimated using linear approximation close to x\n",
    "\n",
    "$f'(x+h) = 0$ , that is  $f'(x) + f''(x)h = 0$, that is $h = -\\frac{f'(x)}{f''(x)}$\n",
    "\n",
    "**Algorithm in 1-Dimension:**\n",
    "\n",
    "$$x_0 = initial\\ guess$$\n",
    "$$h = -\\frac{f'(x)}{f''(x)}$$\n",
    "\n",
    "$$x_{k+1}=x_{k} + h = x_{k}-\\frac{f'(x)}{f''(x)}$$\n",
    "\n",
    "**Algorithm in *N*-Dimension:**\n",
    "$$x_0 = initial\\ guess$$\n",
    "Solve $$J_k s_k = -f(x_k)$$ where $J_k$ is the Jacobian matrix, and $(J)_{ij} = \\frac{∂f_i}{∂x_j}$\n",
    "\n",
    "Update $$x_{k+1}=x_{k} + s_k$$\n",
    "\n",
    "**Convergence**\n",
    "- Quadratic convergence\n",
    "- May fail and reach a local convergence\n",
    "\n",
    "We will release the ability to use BitterDispute to resolve a like optimization use case in future versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNOX_heyvAgK"
   },
   "source": [
    "**References**\n",
    "\n",
    "[1] Derivative. Wikipedia. Available at: https://en.wikipedia.org/wiki/Derivative#History\n",
    "\n",
    "[2] Hoffman, Philipp H.W. “A Hitchhiker’s Guide to Automatic Differentiation.” Numerical Algorithms, 72, 24 October 2015, 775-811, Springer Link, DOI 10.1007/s11075-015-0067-6. \n",
    "\n",
    "[3] Automatic differentiation. Wikipedia. Available at https://en.wikipedia.org/wiki/Automatic_differentiation.\n",
    "\n",
    "[4] Griewank, A., in Complexity in Nonlinear Optimization (ed. Pardalos, P.), World Scientific, Singapore, 1993, pp. 128–161.\n",
    "\n",
    "[5] Coleman, T. F. and Verma, A., in Computational Differentiation: Techniques, Applications and Tools (eds Berz, M., Bischof, C., Corliss, G. and Griewank, A.), SIAM, Philadelphia, 1966, pp. 149–159.\n",
    "\n",
    "[6] Andreas Griewank: Evaluating Derivatives. SIAM 2000.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": " 207 of Diamonds - Milestone 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
