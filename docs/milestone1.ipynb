{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-EW0E8Qe_MB"
   },
   "source": [
    "**Introduction**\n",
    "\n",
    "In the mid-17th century, Isaac Newton and Gottfried Leibniz independently discovered calculus. These groundbreaking scientific advances unfortunately led to a bitter dispute between the two that spanned the duration of their lives. [1] While the origins of calculus may be disputed, its applications are not. Differentiation allows us to identify the maxima, minima and zeros for that function. The ability to do each of these things is crucial in the context of optimization and modern machine learning.\n",
    "\n",
    "The classical way of computing the derivative of a function spans two distinct approaches: approximate, numerical methods and symbolic methods. Each of these methods suffers from its own pitfalls, including numerical instability and long computation time. These pitfalls are magnified as our functions increase in complexity. [2] Automatic Differentiation (AD) suffers from neither instability nor long computational time, and it computes derivatives up to machine precision. AD is easily implemented via computer code, and our package will allow the user to implement to forward mode of Automatic Differentiation in python. [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCGVORb5fWbB"
   },
   "source": [
    "**Background**\n",
    "\n",
    "\n",
    "***What is automatic differentiation (AD)?***\n",
    "\n",
    "\n",
    "Automatic differentiation (AD) is also known as algorithmic differentiation or computational differentiation [1].\n",
    "\n",
    "AD is a set of techniques for numerically evaluating derivatives (gradients) by executing a sequence of arithmetic operations and elementary functions. The derivatives can therefore be computed automatically when applying chain rules to such a sequence of operations [2-3]. There are two major modes of AD: forward and reverse [1-3]. \n",
    "\n",
    "***Why is AD important?***\n",
    "\n",
    "AD and symbolic differentiation both result in more accurate computation than numerical difference estimations. However, unlike symbolic approach, AD evaluates expressions numerically at particular numeric values and does not construct symbolic expressions [1].\n",
    "\n",
    "***How AD works?***\n",
    "\n",
    "The core of AD is the chain rule from Calculus. Chain rule computes the derivative for composition of two or more functions, where the derivative of a function measures the change of the output value relative to the change in input.\n",
    "For a composition of f and h, its derivative can be calculated as the following:\n",
    "        $$\\frac{d}{dx}[f(h(x))] = f'(h(x))h'(x)$$\n",
    "\n",
    "With the help of a computational graph and its traces, partial derivatives relative to x and y are combinations of derivatives of elementary functions, which can be calculated analytically.\n",
    "Let’s consider a simple example: $$f = xcos(y)+xy$$\n",
    "In AD, its computational graph and evaluation trace for forward mode looks like the following:\n",
    "\n",
    "**Computational Graph:**\n",
    "\n",
    "![Computational Graph](BitterDispute_ADGraph.png)\n",
    "\n",
    "**Evaluation Trace:**\n",
    "\n",
    "| Trace   | Elementary Function      | Current Value                   | Elementary Function Derivative       | $\\nabla_{x}$ Value  | $\\nabla_{y}$ Value  |\n",
    "| :---: | :-----------------: | :----------------------: | :----------------------------: | :-----------------:  | :-----------------: |\n",
    "| $x_{1}$ | $x_{1}$                  | $x$                     | $\\dot{x}_{1}$                     | $1$ | $0$ |\n",
    "| $x_{2}$ | $x_{2}$                  | $y$                     | $\\dot{x}_{2}$                     | $0$ | $1$ |\n",
    "| $x_{3}$ | $cos(x_{2})$             | $cos(y)$                | $-sin(x_{2})\\dot{x}_{2}$          | $0$ | $-sin(y)$ |\n",
    "| $x_{4}$ | $x_{1}x_{2}$             | $xy$           | $\\dot{x}_{1}x_{2}+x_{1}\\dot{x}_{2}$      | $y$ | $x$ |\n",
    "| $x_{5}$ | $x_{1}cos(x_{2})$        | $xcos(y)$       | $\\dot{x}_{1}(-sin(x_{2}))+x_{1}\\dot{x}_{2}$|$-sin(y)$ | $x$ |\n",
    "| $x_{6}$ | $x_{3}+x_{5}$        | $cos(y)+xcos(y)$       | $\\dot{x}_{3}+\\dot{x}_{5}$    | $-sin(y)$ | $-sin(y)+x$ |\n",
    "\n",
    "\n",
    "We found that $$\\frac{d}{dx}f=-sin(y)$$ and $$\\frac{d}{dy}f=sin(y)+x$$\n",
    "In forward mode, the values and their derivatives are stored along the chain accumulatively.\n",
    "\n",
    "***Applications of AD:***\n",
    "\n",
    "AD has been used in many applications, including optimization (solving nonlinear equations utilizing gradients/Hessians), inverse problems/data assimilation, neural networks, etc. [4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nd2JqNHvf3bI"
   },
   "source": [
    "**How to Use BitterDispute**\n",
    "\n",
    "*How do we envision that a user will interact with our package? What should they import? How can they instantiate AD objects?*\n",
    "\n",
    "First, a user should be able to download our package using pip or conda. Our instructions will include steps on creating a custom environment to ensure our package runs effectively. We will encourage the use of conda for the creation and activation of our custom environment but will aim to allow other tools like pipenv or virtualenv. We will provide the required package version dependencies in a file called AD_requirements.txt. Some of these dependencies will include open-source packages like numpy, scipy, pandas or math to provide a user with more flexibility in their function definition and to be used by our software during differentiation calculations.\n",
    "\n",
    "    ‘pip install BitterDispute' or ‘conda install BitterDispute'\n",
    "\n",
    "After installation in a user’s environment, the user will be able to import our AD package into their current Python, Jupyter or other session using the simple command:\n",
    "\n",
    "    from BitterDispute import AD\n",
    "  \n",
    "We intend to bundle all functionality into a single module for the user to import. Within that module, we will have multiple callable properties for the user to call when they need to retrieve specific information about the input formula and the derived value or formula.\n",
    "\n",
    "A user will initiate automatic differentiation by instantiating the class with a single command ***AD( )***. From there, the software package will begin to print a series of instructional steps for the user to follow, including inputting the number of formula variables, the formula itself written in terms of those variables and hard-coded values for each of those variables that the user wishes to derive the equation at. These input parameters will be saved as a series of properties on the object, for later reference by the user if desired. The object will print a statement summarizing the formula and variable values and return to output the derived value based on the inputted numbers.\n",
    "If the user wishes to reference these values later, they will be saved in callable properties and the user will need to save an instance of our class into a local variable. This can be done using commands such as ***‘X = AD( )’***. After that, the user will be able to reference saved parameters such as:\n",
    "\n",
    "    X.derivative   (Outputted value based on variable value inputs)\n",
    "\n",
    "    X.values       (Variable value inputs)\n",
    "\n",
    "    X.formula      (String representing formula input)\n",
    "\n",
    "    X.trace_count  (Count of trace steps necessary during derivation)\n",
    "\n",
    "\n",
    "We want to provide advanced users of our package with the ability to accelerate through steps if they understand our package’s functionality. To do this, we intend to allow a user to enter an arbitrary number of parameters, where the first is the formula and the remainder are of type int or float during instantiation, using a command like ***AD(1, 4, 8, 3)***. This will automatically derive answers to the question about how many variables are in the formula and will assign the input values to those variables in the order they appear in the formula from left to right.\n",
    "\n",
    "Lastly, we are considering providing the user with the ability to re-use an instance of our package with a saved formula. If a user has saved an instance to a local variable (like X from earlier), the user will be able to call X(x, y) where x and y are two values of type int or float and X is an instantiation of AD with a saved formula of just two variables. Calling an existing instance will update the X.derivative value with the new derived value, keep the saved formula and update the input values saved in X.values. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eu_rwCISgFQE"
   },
   "source": [
    "**Software Organization**\n",
    "\n",
    "*What will the directory structure look like?*\n",
    "\n",
    "    /bitterdispute\n",
    "        __init__.py\n",
    "        README.md\n",
    "        LICENSE\n",
    "        requirements.txt\n",
    "        forward_mode/\n",
    "            __init__.py\n",
    "            AD.py\n",
    "            AD_scalars.py\n",
    "            AD_functions.py\n",
    "            AD_vectors.py\n",
    "        optimization/\n",
    "            __init__.py\n",
    "            AD_optimum.py\n",
    "        tests/\n",
    "            __init.py\n",
    "            tests_forward.py\n",
    "            tests_optimization.py\n",
    " \n",
    "*What modules do you plan on including? What is their basic functionality?*\n",
    "\n",
    "Without our package */bitterdispute*, we will have separate modules for our forward mode implementation, optimization extension and test cases. Our forward mode module will have all necessary classes and functions to execute forward mode automatic differentiation. Our optimization module will include all code extending forward mode against an optimization use case. Lastly, our tests module will include all tests run against both modules.\n",
    "\n",
    "*Where will your test suite live? Will you use TravisCI? CodeCov?*\n",
    "\n",
    "Our tests will live in a dedicated module to assist with robust test creation. We will be using both Travis-CI and CodeCov to monitor our commits and to ensure that keeping our tests in a separate module doesn’t prevent us from maintaining sufficient code coverage.\n",
    "\n",
    "*How will you distribute your package (e.g. PyPI)?*\n",
    "We intend to use the PyPI package distribution mechanism.\n",
    "\n",
    "*How will you package your software? Will you use a framework? If so, which one and why? If not, why not?*\n",
    "\n",
    "At this time, we do not intend to use a framework but do wish to research Flask as a potential option as we further develop our software organization and implementation. Flask would increase our ability to build interactivity, is simpler to get started with than its primary alternative Django and is more explicit, which will help us define our automatic differentiation steps in a clear manner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kQj04h6CgUse"
   },
   "source": [
    "**Implementation (@Selina)**\n",
    "\n",
    "Discuss how you plan on implementing the forward mode of automatic differentiation.\n",
    "What are the core data structures?\n",
    "\n",
    "Parent class **AD ()**: call one of the three classes\n",
    "\n",
    "\n",
    "\n",
    "> scalar values: **AD_scalars**\n",
    "\n",
    "First create a *variable object* **x** and then define the symbolic expression for *function* **f**. \n",
    "\n",
    "```\n",
    ">>> a = 2.0\n",
    ">>> x = autodiff.Variable(a)\n",
    ">>> alpha = 2.0\n",
    ">>> beta = 3.0\n",
    ">>> f = alpha * x + beta\n",
    "```\n",
    "Where **f.der** contains derivative to 'x', **f.val** contains function value\n",
    "\n",
    "\n",
    "```\n",
    ">>> print(\"alpha * x + beta = \", f.val, \"; Derived value for f = \", f.der)\n",
    "alpha * x + beta =  7.0 ; Derived value for f =  2.0\n",
    "\n",
    "```\n",
    "\n",
    "If *variable object* is a special function\n",
    "\n",
    "\n",
    "```\n",
    ">>> a = 4.0\n",
    ">>> x = autodiff.Variable(a)\n",
    ">>> f = 7 * autodiff.Sin(x) + 3 \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> scalar functions of vectors: **AD_functions**\n",
    "\n",
    "First create two *variable objects* **x1, x2** and then define the symbolic expression for *function* **f**. \n",
    "```\n",
    "a1 = 4.0\n",
    "a2 = 3.0\n",
    "x1 = autodiff.Variable(a1, name='x1')\n",
    "x2 = autodiff.Variable(a2, name='x2')\n",
    "f = x1 - x2 * x_1\n",
    "\n",
    "```\n",
    "\n",
    "Where f.der is a dictionary that contains derivative to ‘x1’ and derivative to ‘x2’ \n",
    "\n",
    "\n",
    "```\n",
    ">>>  print(f.der)\n",
    "{'x1': -3, 'x2': -4}\n",
    ">>>  print(f.der['x1'])\n",
    "-3\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> vector functions of vectors, **AD_vectors**\n",
    "\n",
    "For example: \n",
    "$$f_1(x_1, x_2) = x_1 - x_2*x_1$$\n",
    "$$f_2(x_1, x_2) = \\frac{x_1}{x_2}$$\n",
    "\n",
    "$$f(x_1, x_2) = (f_1(x_1, x_2), f_2(x_1, x_2))$$\n",
    "\n",
    "```\n",
    "a1 = 4.0\n",
    "a2 = 3.0\n",
    "x1 = autodiff.Variable(a1, name='x1')\n",
    "x2 = autodiff.Variable(a2, name='x2')\n",
    "f1 = x1 - x2 * x_1\n",
    "f2 = x1 / x2\n",
    "\n",
    "\n",
    "Jacobian (f1 f2) = f1.der(on='x1'), f1.der(on='x2') = (f1.der, f2.der)\n",
    "                   f2.der(on='x1'), f2.der(on='x2')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GKiyMA3QBkd"
   },
   "source": [
    "What classes will you implement?\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "class Variable:\n",
    "  \n",
    "  def __init__(self, value, der=1, name='x'):\n",
    "    \"\"\"Claim variable\n",
    "    \n",
    "    INPUTS\n",
    "    =======\n",
    "    value: float, \n",
    "           value of claimed variable\n",
    "    der  : float, optional, default value is 1\n",
    "           derivative of claimed variable\n",
    "    name : string, optional, default value is 'x'\n",
    "           name of claimed variable\n",
    "    \n",
    "    RETURNS\n",
    "    ========\n",
    "    Variable: Variable object\n",
    "              contains parameters val, der, and name\n",
    "    \n",
    "    EXAMPLES\n",
    "    =========\n",
    "    >>> x = autodiff.Variable(a1, name='x1')\n",
    "    >>> print(x.val, x.der, x.name)\n",
    "    (2.0, {'x': 1}, 'x')\n",
    "    \"\"\"\n",
    "        \n",
    "  def der(self, on=\"x\"):\n",
    "    \"\"\"\n",
    "    Return the derivative on given variable name\n",
    "    \"\"\"\n",
    "    \n",
    "  def __add__(self, other):\n",
    "    \"\"\"\n",
    "    Add two Variable objects : self and other,\n",
    "    Return a new Variable object\n",
    "    \"\"\"\n",
    "  \n",
    "class Sin:\n",
    "\n",
    "  def __init__(self, x):\n",
    "    \"\"\"Claim a special function\n",
    "    \n",
    "    INPUTS\n",
    "    =======\n",
    "    x    : Variable object, \n",
    "           A pre-claimed variable\n",
    "    \n",
    "    RETURNS\n",
    "    ========\n",
    "    element: Special element object\n",
    "             contains parameters val, der, and name\n",
    "    \n",
    "    EXAMPLES\n",
    "    =========\n",
    "    >>> x = autodiff.Variable(a1, name='x1')\n",
    "    >>> s = autodiff.Sin(x)\n",
    "    >>> print(s.val, s.der, s.name)\n",
    "    (2.0, {'x': cos(1)}, 'x')\n",
    "    \"\"\"\n",
    "  \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRIZ9ZF89NNV"
   },
   "source": [
    "\n",
    "What method and name attributes will your classes have?\n",
    "\n",
    "* Methods\n",
    "\n",
    "```\n",
    "__add__\n",
    "__radd__\n",
    "__sub__\n",
    "__rsub__\n",
    "__mul__\n",
    "__rmul__\n",
    "\n",
    "``` \n",
    "What external dependencies will you rely on?\n",
    "\n",
    "\n",
    "```\n",
    "Numpy\n",
    "\n",
    "Math\n",
    "\n",
    "scipy*\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdBwsz52IeAV"
   },
   "source": [
    "How will you deal with elementary functions like ***sin, sqrt, log, and exp*** (and all the others)?\n",
    "\n",
    "> Element function class : **autodiff.Sin(x)**\n",
    "\n",
    "\n",
    "```\n",
    "f = 2 * autodiff.Sin(x) + 3\n",
    "\n",
    "print(f.val, f.der)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNOX_heyvAgK"
   },
   "source": [
    "**References**\n",
    "\n",
    "[1] Automatic differentiation. Wikipedia. Available at https://en.wikipedia.org/wiki/Automatic_differentiation.\n",
    "\n",
    "[2] Griewank, A., in Complexity in Nonlinear Optimization (ed. Pardalos, P.), World Scientific, Singapore, 1993, pp. 128–161.\n",
    "\n",
    "[3] Coleman, T. F. and Verma, A., in Computational Differentiation: Techniques, Applications and Tools (eds Berz, M., Bischof, C., Corliss, G. and Griewank, A.), SIAM, Philadelphia, 1966, pp. 149–159.\n",
    "\n",
    "[4] Andreas Griewank: Evaluating Derivatives. SIAM 2000.\n",
    "\n",
    "[5] Derivative. Wikipedia. Available at: https://en.wikipedia.org/wiki/Derivative#History\n",
    "\n",
    "[6] Hoffman, Philipp H.W. “A Hitchhiker’s Guide to Automatic Differentiation.” Numerical Algorithms, 72, 24 October 2015, 775-811, Springer Link, DOI 10.1007/s11075-015-0067-6. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "207 of Diamonds - Milestone 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
